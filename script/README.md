# Sensitivity Analysis and Automatic Pddl Problem Solving Tools

## Scripts

Four scripts compose the workflow for the automatic generation and run of pddl problems
and its relative sensitivity analysis for the identification of the optimal wA* parameters.
The scripts can be launched sequentially to perform the full evaluation of a single or multiple problems,
but can also be used independently if need.

### test_data.py

```
python3 test_data.py (--opt-args)
```
Script that:
1. generates a fixed number of problems among all possible problems with a maximum amount of drinks per table and total drinks ordered; it avoids repetition of cases checked for in the past by looking at their rercord in the file `drinks_explored.pkl` under the folder `lib` (which is updated each time with the new ones)
2. runs them all with a collection of hw, gw values
3. parses the output of all runs for all problems and saves everything in a csv file

The command line parameters can be used to set:
- the number of random problems to generate and test
- the timeout for each run

> This script should not be run directly from the end user ideally (assuming a completely trained model).

### build.py

```
python3 build.py (--opt-args)
```
This script can be used to generate a pddl problem file fully compatible with the "APE full" domain
(aka using actions, processes and events implementing the full specifics of the assignment).

The optinal input arguments of this script are:
- *-f*: problem name and path        
- *-w*: number of waiters                 
- *-d*: number of total drinks per table
- *-t*: number of hot drinks per table
- *-g*: GUI trigger for a user friendy inputs insertion 

If launched with no parameters, the scripts generate a default test problem.

### run.py

```
python3 run.py (--opt-args)
```
This scripts automatically runs multiple instances of a specified .pdd problem with the
"APE full" domain via the enhsp solver.
The problem is solved consigering a series of different values of the h and g weights 
for the used A* heuristic algorithm.

If launched with no parameters a default run with the default test problem will be performed, and the results
will be saved in a .txt file (by default in the _output_ folder)

This script is where the magic happens: by passing `-M` as a command line flag the system will autonomously find what it
assumes to be the best [hw,gw] (or, at least, the best values among a finite list it has). This result is obtained thanks
to a Linear Regression smart agent trained on random configurations, for which it has a record of the quality of the solution
for a set of [hw, gw] (_see_ `test_data.py` _and_ `correlation.py` _for more info_).
The file to run is parsed in order to gather details on it's structure (_number of waiters, total drinks and hot drinks, metrics on their position_), which are then passed as input to the set of Linear Regression models in order to find an approximation of the quality of the solution for each weight couple [hw, gw]. The couple yielding the best expected quality is thus selected and used for the run.

With the input parameters is possible to set:
- domain name and location
- problem name and location
- output file name and location
- maximum time to run each instance for
- h and g sets of weights
- approximation of the best [hw, gw] values to start with

### parse.py

```
python3 parse.py (--opt-args)
```
With this script it's possibile to parse an output file generated by `run.py`, composing a dictionary that
stores, for each "interesting" parameter of the solution, the values obtained for it by varying **h** and **g**.
That relation is thus plotted and saved as pdf in the folder _graphs_.

With the parameters is possible to set:
- name and location of the file to be parsed
- plots can be set to 3D view instead of the default 2D+color
- LaTeX style can be adopted in those graphs (requires `sudo apt-get install texlive-full`)

> This script should not be run directly from the end user ideally.

### correlation.py

```
python3 correlation.py
```
This script is the one that can be used to train a collection of Linear Regression models; two models are trained for each pair of [hw, gw], one learning to predict the solution's "Duration" and one the "Search Time" that the planner will take to find it.
The dictionary of models is saved in a pickle file, `regr_model.pkl` under the folder `lib`, and is accessed by `run.py` when the flag `-M` is passed to it. Notice that, actually, what the model learns are the logarithms of such two metrics, a relation that was decided after inspecting the data trends for multiple runs (which can be plotted with an appropriate flag)

Here the cmd parameters can be used to:
- pass the path and name of the csv to read data from
- toggle the plotting of the figures demonstrating the correlation between some of the problem's features
- save such plots as pdf files
- specify for which h,g values to plot data (default is [1.0, 1.0])

> This script should not be run directly from the end user ideally (assuming a completely trained model)..

### data_util.py

```
python3 data_util.py
```

This script simply contains some util functions.

---

## Dependencies

Python3 is needed to launch those scripts.
Required python libraries:
  - numpy
  - pandas
  - matplotlib
  - seaborn
  - sklearn
  - tertools
  - subprocess
  - statsmodels
  - PySimpleGUI

ENHSP-20 compiled from source is also needed to solve the domain-problem couple. By default the position of the planner executable
is assumed at `/root/ENHSP-20`, to change it open `run.py` and modify the `engine_path` global variable.
Notice that the off-the-shelf compiled version was discarded due to it not managing to set hw, gw appropriately (despite passing them from command line)



